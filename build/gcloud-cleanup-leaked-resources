#!/usr/bin/env bash

# This cleans up google cloud resources that are more than 24 hours old. It is intended to be run on a regular schedule (once a day) in Team City but it can be
# run manually by a developer as well.
#
# If developers want long-lived clusters they must use a different Google Cloud project to the one that CI uses!

# make bash play nicely
#
set -o pipefail -o errtrace -o errexit -o nounset
shopt -s inherit_errexit
[[ -n "${TRACE:-}" ]] && set -o xtrace

# Required env vars
CLOUDSDK_CORE_PROJECT="${CLOUDSDK_CORE_PROJECT:?CLOUDSDK_CORE_PROJECT is required}"

# disable delete prompts
export CLOUDSDK_CORE_DISABLE_PROMPTS=True

# unlink ourselves from whatever cluster was just being used
unset CLOUDSDK_CONTAINER_CLUSTER

GCR_REPO="eu.gcr.io/${CLOUDSDK_CORE_PROJECT}/neo4j-helm-chart"
CUT_OFF_TIMESTAMP="$(expr $(date +%s) - 86400)"

# convert to YYY-MM-DD date, taking into account different implementations of date on osx vs debian
# do not try and be more specific than the date, different gcloud resources use different formats beyond the date so this is easiest
CUT_OFF_DATE="$(date -r ${CUT_OFF_TIMESTAMP} "+%Y-%m-%d" 2>/dev/null || date -d @${CUT_OFF_TIMESTAMP} "+%Y-%m-%d")"

echo "Removing all items created before ${CUT_OFF_DATE} (${CUT_OFF_TIMESTAMP})"

# Container Registry Images
gcloud container images list-tags "${GCR_REPO}" --filter="timestamp.datetime<${CUT_OFF_DATE}" --format="get(digest)" \
  | xargs -I{} gcloud container images delete "${GCR_REPO}@{}" --force-delete-tags

# Kubernetes Clusters
#read the cluster name and zone in an array and trigger the cluster delete command
readarray -t clusters_list < <(gcloud container clusters list --filter="createTime<${CUT_OFF_DATE}" --format='value[separator=","](name,location)')
for i in "${clusters_list[@]}"
do
   cluster_name=$(echo $i | awk -F "," '{print $1}')
   cluster_zone=$(echo $i | awk -F "," '{print $2}')
   echo "Deleting cluster ${cluster_name} in zone ${cluster_zone}"
   gcloud container clusters delete "${cluster_name}" --zone "${cluster_zone}" &
done
#wait for the above clusters to be deleted
wait


# Compute Disks (have to come after clusters)
gcloud compute disks list --filter="creationTimestamp<${CUT_OFF_DATE}" --format="get(selfLink)" \
  | cut -d'/' -f6- | xargs -I{} gcloud compute disks delete {}

# Google Cloud Filestore instances
gcloud filestore instances list --filter="createTime<${CUT_OFF_DATE}" --format="get(name)" \
  | xargs -I{} gcloud filestore instances delete {}

function count_leaked_resources {
  gcloud asset search-all-resources --scope="projects/${CLOUDSDK_CORE_PROJECT}" | wc -l
}

